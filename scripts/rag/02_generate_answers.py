import argparse, csv, os, yaml
from typing import List, Dict, Tuple
from dotenv import load_dotenv
from pathlib import Path
from tqdm import tqdm


# –æ–±—â–∏–π –±–∏–ª–¥–µ—Ä –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Ç–æ–Ω–∫–∞—è –æ–±—ë—Ä—Ç–∫–∞ –Ω–∞–¥ _ctx_impl.build_context)
from _ctx import build as build_context


# =========================
#  LLM backends
# =========================

def call_llm_openai(system_prompt: str,
                    user_prompt: str,
                    model_name: str,
                    max_tokens: int = 400,
                    temperature: float = 0.2) -> str:
    """–í—ã–∑–æ–≤ –º–æ–¥–µ–ª–∏ OpenAI"""
    try:
        import openai
        client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        resp = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_prompt},
            ],
            max_tokens=max_tokens,
            temperature=temperature,
        )
        return (resp.choices[0].message.content or "").strip()
    except Exception as e:
        return f"[ERROR calling OpenAI: {e}]"


def call_llm_gemini(system_prompt: str,
                    user_prompt: str,
                    model_name: str = "gemini-1.5-flash",
                    max_output_tokens: int = 400,
                    temperature: float = 0.2) -> str:
    """–í—ã–∑–æ–≤ Google Gemini API (–Ω–æ–≤—ã–π v1)"""
    try:
        from google import genai
        from google.genai import types
        client = genai.Client(api_key=os.getenv("GEMINI_API_KEY"))
        response = client.models.generate_content(
            model='gemini-2.0-flash',
            contents=f"{system_prompt}\n\n{user_prompt}",
            config=types.GenerateContentConfig(
                temperature=temperature,
                max_output_tokens=max_output_tokens
            ),
        )
        return (response.text or "").strip()
    except Exception as e:
        return f"[ERROR calling Gemini: {e}]"



def call_llm_local_stub(system_prompt: str,
                        user_prompt: str,
                        **_) -> str:
    """–û—Ñ—Ñ–ª–∞–π–Ω-–∑–∞–≥–ª—É—à–∫–∞"""
    return (user_prompt[:1200] + "\n\n[local stub: replace with OpenAI or Gemini]").strip()


# =========================
#  Prompts
# =========================

PROMPT_SYS = (
    "–¢—ã ‚Äî —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –Ω–µ—Ñ—Ç–µ–≥–∞–∑–æ–≤–æ–π –æ—Ç—Ä–∞—Å–ª–∏ –ö–∞–∑–∞—Ö—Å—Ç–∞–Ω–∞. "
    "–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ, –ø–æ –¥–µ–ª—É, –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç. "
    "–ï—Å–ª–∏ –¥–∞–Ω–Ω—ã—Ö –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ ‚Äî —á–µ—Å—Ç–Ω–æ —É–∫–∞–∂–∏ —ç—Ç–æ. –°–æ—Ö—Ä–∞–Ω—è–π —è–∑—ã–∫ –≤–æ–ø—Ä–æ—Å–∞."
)

PROMPT_USER_TMPL = (
    "–í–æ–ø—Ä–æ—Å:\n{q}\n\n"
    "–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{ctx}\n\n"
    "–¢—Ä–µ–±–æ–≤–∞–Ω–∏–µ:\n"
    "- –ò—Å–ø–æ–ª—å–∑—É–π —Ñ–∞–∫—Ç—ã —Ç–æ–ª—å–∫–æ –∏–∑ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞.\n"
    "- –ï—Å–ª–∏ –≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º ‚Äî –æ—Ç–≤–µ—á–∞–π –ø–æ-—Ä—É—Å—Å–∫–∏; –µ—Å–ª–∏ –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º ‚Äî –ø–æ-–∞–Ω–≥–ª–∏–π—Å–∫–∏.\n"
    "- –ü–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ —É–∫–∞–∂–∏ –∏—Å—Ç–æ—á–Ω–∏–∫(–∏) –≤ –∫–æ–Ω—Ü–µ –∫—Ä–∞—Ç–∫–æ.\n"
)


# =========================
#  IO helpers
# =========================

def detect_question_column(fieldnames: List[str]) -> str:
    """–û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–æ–ª–æ–Ω–∫—É —Å –≤–æ–ø—Ä–æ—Å–æ–º"""
    lower = [c.lower() for c in fieldnames]
    if "question" in lower:
        return fieldnames[lower.index("question")]
    if "–≤–æ–ø—Ä–æ—Å" in lower:
        return fieldnames[lower.index("–≤–æ–ø—Ä–æ—Å")]
    return fieldnames[0]


def collect_sources(items: List[Dict]) -> Tuple[str, str]:
    """–°–æ–±–∏—Ä–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –∏ chunk_id"""
    srcs = []
    chunk_ids = []
    seen = set()
    for it in items:
        s = it.get("source", "")
        if s and s not in seen:
            seen.add(s)
            srcs.append(s)
        chunk_ids.append(it.get("chunk_id", ""))
    return " | ".join(srcs), " | ".join(chunk_ids)


# =========================
#  Main
# =========================

def main():
    ap = argparse.ArgumentParser()
    base_dir = Path(__file__).resolve().parents[2]
    ap.add_argument("--input", default=base_dir / "submission_template.csv", help="–ü—É—Ç—å –∫ CSV —Å –≤–æ–ø—Ä–æ—Å–∞–º–∏")
    ap.add_argument("--output", required=True, help="–ö—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å CSV —Å –æ—Ç–≤–µ—Ç–∞–º–∏")
    ap.add_argument("--config",  default=base_dir / "config.yaml", help="–ü—É—Ç—å –∫ config.yaml")
    args = ap.parse_args()

    cfg = yaml.safe_load(open(args.config, "r", encoding="utf-8"))

    provider = (cfg.get("model", {}).get("provider") or "local").lower()
    model_name = cfg.get("model", {}).get("name", "gpt-4o-mini")
    max_tokens = int(cfg.get("model", {}).get("max_tokens", 400))
    temperature = float(cfg.get("model", {}).get("temperature", 0.2))

    # =========================
    #  –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–π
    # =========================
    env_path = Path(__file__).resolve().parents[2] / ".env"
    load_dotenv(dotenv_path=env_path)

    if provider == "openai":
        if os.getenv("OPENAI_API_KEY"):
            print("üîë OpenAI API key loaded successfully.")
        else:
            print("‚ö†Ô∏è  OPENAI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–µ—Ä–µ—Ö–æ–∂—É –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –∑–∞–≥–ª—É—à–∫—É.")
            provider = "local"

    elif provider == "gemini":
        if os.getenv("GEMINI_API_KEY"):
            print("üîë Gemini API key loaded successfully.")
        else:
            print("‚ö†Ô∏è  GEMINI_API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω. –ü–µ—Ä–µ—Ö–æ–∂—É –Ω–∞ –ª–æ–∫–∞–ª—å–Ω—É—é –∑–∞–≥–ª—É—à–∫—É.")
            provider = "local"

    else:
        print("‚ÑπÔ∏è  Provider set to 'local' ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º –æ—Ñ—Ñ–ª–∞–π–Ω —Ä–µ–∂–∏–º.")

    # =========================
    #  –í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏
    # =========================
    if provider == "gemini":
        llm = lambda q, c: call_llm_gemini(PROMPT_SYS, PROMPT_USER_TMPL.format(q=q, ctx=c),
                                           model_name, max_tokens, temperature)
    elif provider == "openai":
        llm = lambda q, c: call_llm_openai(PROMPT_SYS, PROMPT_USER_TMPL.format(q=q, ctx=c),
                                           model_name, max_tokens, temperature)
    else:
        llm = lambda q, c: call_llm_local_stub(PROMPT_SYS, PROMPT_USER_TMPL.format(q=q, ctx=c))

    # =========================
    #  –ß—Ç–µ–Ω–∏–µ CSV –∏ –ø—Ä–æ–≥—Ä–µ—Å—Å
    # =========================
    with open(args.input, "r", encoding="utf-8") as f:
        reader = csv.DictReader(f)
        qcol = detect_question_column(reader.fieldnames)

        out_rows = []
        rows_iter = list(reader)
        for r in tqdm(rows_iter, desc="üß© Processing questions", ncols=100):
            q = (r.get(qcol) or "").strip()
            if not q:
                continue

            ctx, items = build_context(q, args.config)
            ans = llm(q, ctx)
            sources_str, chunk_ids_str = collect_sources(items)

            out_rows.append({
                "question": q,
                "answer": ans,
                "sources": sources_str,
                "chunk_ids": chunk_ids_str
            })

    # =========================
    #  –ó–∞–ø–∏—Å—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
    # =========================
    fieldnames = ["question", "answer", "sources", "chunk_ids"]
    with open(args.output, "w", encoding="utf-8", newline="") as f:
        writer = csv.DictWriter(f, fieldnames=fieldnames)
        writer.writeheader()
        writer.writerows(out_rows)

    print(f"‚úÖ Saved: {args.output}  (rows: {len(out_rows)})")


if __name__ == "__main__":
    main()
